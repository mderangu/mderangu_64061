{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Applying RNN with embedding layer"
      ],
      "metadata": {
        "id": "nh-R8Zx7xcgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Load the IMDB dataset\n",
        "vocab_size = 10000  # Considering only the top 10,000 words\n",
        "max_words = 150  # Cutoff reviews after 150 words\n",
        "maxlen = 150\n",
        "training_samples = 100  # Restrict training samples to 100\n",
        "validation_samples = 10000\n",
        "\n",
        "# Load data, restrict to top_words and set max_words\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Preprocess data\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Prepare model\n",
        "embedding_dim = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train[:training_samples], y_train[:training_samples],\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_train[training_samples:training_samples+validation_samples],\n",
        "                                     y_train[training_samples:training_samples+validation_samples]))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA5Cj2ju-1dG",
        "outputId": "fdac31aa-0794-4964-8ccd-061ebac4fc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 7s 1s/step - loss: 0.6901 - acc: 0.5800 - val_loss: 0.6953 - val_acc: 0.4948\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 3s 897ms/step - loss: 0.6827 - acc: 0.5800 - val_loss: 0.6949 - val_acc: 0.4948\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 3s 896ms/step - loss: 0.6798 - acc: 0.5800 - val_loss: 0.6972 - val_acc: 0.4948\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.6736 - acc: 0.5800 - val_loss: 0.6975 - val_acc: 0.4948\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 3s 1s/step - loss: 0.6688 - acc: 0.5800 - val_loss: 0.6962 - val_acc: 0.4948\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 3s 909ms/step - loss: 0.6634 - acc: 0.5800 - val_loss: 0.7008 - val_acc: 0.4948\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 3s 914ms/step - loss: 0.6541 - acc: 0.5800 - val_loss: 0.7117 - val_acc: 0.4948\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 3s 1s/step - loss: 0.6447 - acc: 0.5800 - val_loss: 0.7232 - val_acc: 0.4948\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 3s 901ms/step - loss: 0.6307 - acc: 0.5800 - val_loss: 0.7265 - val_acc: 0.4948\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 3s 902ms/step - loss: 0.6136 - acc: 0.5800 - val_loss: 0.7073 - val_acc: 0.4950\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.7046 - acc: 0.5000\n",
            "Test Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving Performance with Limited Data"
      ],
      "metadata": {
        "id": "w4b5dKy3LlXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout\n",
        "\n",
        "# Load the IMDB dataset\n",
        "vocab_size = 10000  # Considering only the top 10,000 words\n",
        "max_words = 150  # Cutoff reviews after 150 words\n",
        "maxlen = 150\n",
        "training_samples = 100  # Restrict training samples to 100\n",
        "validation_samples = 10000\n",
        "\n",
        "# Load data, restrict to top_words and set max_words\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Preprocess data\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Prepare model\n",
        "embedding_dim = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(SpatialDropout1D(0.2))  # Spatial dropout to reduce overfitting\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.1))  # Dropout to reduce overfitting\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Train the model with data augmentation\n",
        "history = model.fit(x_train[:training_samples], y_train[:training_samples],\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_train[training_samples:training_samples+validation_samples],\n",
        "                                     y_train[training_samples:training_samples+validation_samples]))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_xKctsBJO-A",
        "outputId": "e491b587-c82b-4a52-dca7-54a9b2949e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 5s 989ms/step - loss: 0.6921 - acc: 0.5400 - val_loss: 0.6940 - val_acc: 0.4948\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 3s 876ms/step - loss: 0.6864 - acc: 0.5800 - val_loss: 0.6941 - val_acc: 0.4948\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 3s 887ms/step - loss: 0.6808 - acc: 0.5800 - val_loss: 0.6951 - val_acc: 0.4948\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 3s 1s/step - loss: 0.6755 - acc: 0.5800 - val_loss: 0.6974 - val_acc: 0.4948\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 3s 868ms/step - loss: 0.6692 - acc: 0.5800 - val_loss: 0.7023 - val_acc: 0.4948\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 3s 885ms/step - loss: 0.6596 - acc: 0.5800 - val_loss: 0.7032 - val_acc: 0.4948\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 3s 857ms/step - loss: 0.6504 - acc: 0.5800 - val_loss: 0.6998 - val_acc: 0.4948\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 3s 1s/step - loss: 0.6422 - acc: 0.5800 - val_loss: 0.7002 - val_acc: 0.4948\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 3s 915ms/step - loss: 0.6287 - acc: 0.5900 - val_loss: 0.7014 - val_acc: 0.4948\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 3s 895ms/step - loss: 0.6111 - acc: 0.6000 - val_loss: 0.6964 - val_acc: 0.4988\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.6945 - acc: 0.5030\n",
            "Test Accuracy: 0.5030400156974792\n"
          ]
        }
      ]
    }
  ]
}